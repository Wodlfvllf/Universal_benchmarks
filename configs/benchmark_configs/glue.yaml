benchmark:
  name: "GLUE"
  version: "1.0"
  description: "General Language Understanding Evaluation"
  category: "llm"
  paper: "Wang et al., 2018"
  official_url: "https://gluebenchmark.com/"

dataset:
  source: "huggingface"
  path: "nyu-mll/glue"
  cache_dir: "${CACHE_DIR}/project_datasets/glue"
  
subtasks:
  - name: "sst2"
    task_type: "classification"
    dataset_config: "sst2"
    metrics: ["accuracy", "f1"]
    input_columns: ["sentence"]
    label_column: "label"

evaluation:
  batch_size: 32

output:
  save_raw_predictions: true
