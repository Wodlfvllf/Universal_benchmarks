benchmark:
  name: "SuperGLUE"
  version: "1.0"
  description: "SuperGLUE Benchmark"
  category: "llm"
  paper: "Wang et al., 2019"
  official_url: "https://super.gluebenchmark.com/"

dataset:
  source: "huggingface"
  path: "super_glue"
  cache_dir: "${CACHE_DIR}/project_datasets/super_glue"
  
subtasks:
  - name: "copa"
    task_type: "copa"
    model: "gpt2"
    dataset_config: "copa"
    metrics: ["accuracy"]
    input_columns: ["premise", "choice1", "choice2"]
    label_column: "label"

evaluation:
  batch_size: 32

output:
  save_raw_predictions: true