benchmark:
  name: "MMLU"
  task_type: "multiple_choice"
  
subtasks:
  - name: "abstract_algebra"
    subject: "STEM"
    num_choices: 4
    metrics: ["accuracy", "per_subject_accuracy"]
    
evaluation:
  few_shot_examples: 5
  prompt_template: |
    Question: {question}
    Choices:
    A) {choice_a}
    B) {choice_b}
    C) {choice_c}
    D) {choice_d}
    Answer: