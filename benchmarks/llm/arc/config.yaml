# ARC (AI2 Reasoning Challenge) Benchmark Configuration

benchmark:
  name: "llm/arc"
  version: "1.0"
  description: "A benchmark for question answering on science questions."
  category: "llm"
  paper: "Clark et al., 2018"
  official_url: "https://allenai.org/data/arc"

dataset:
  name: "arc"
  source: "huggingface"
  path: "ai2_arc"
  cache_dir: "/root/benchmarks/universal-model-benchmarks/project_datasets/cache"

subtasks:
  - name: "ARC-Challenge"
    task_type: "multiple_choice"
    dataset_config: "ARC-Challenge"
    metrics: ["accuracy"]
    input_columns: ["question", "choices"]
    label_column: "answerKey"

  - name: "ARC-Easy"
    task_type: "multiple_choice"
    dataset_config: "ARC-Easy"
    metrics: ["accuracy"]
    input_columns: ["question", "choices"]
    label_column: "answerKey"

evaluation:
  batch_size: 4
  use_cache: true

output:
  format: "json"
  save_raw_predictions: false
  aggregate_subtasks: true
  output_dir: "/root/benchmarks/universal-model-benchmarks/results/raw"
