# COCO Captions Benchmark Configuration

benchmark:
  name: "vision_language/coco_captions"
  version: "1.0"
  description: "Image captioning on the Common Objects in Context dataset."
  category: "vision_language"
  paper: "Chen et al., 2015"
  official_url: "https://cocodataset.org/"

dataset:
  name: "coco_captions"
  source: "huggingface"
  path: "HuggingFaceM4/COCO"
  cache_dir: "/root/benchmarks/universal-model-benchmarks/project_datasets/cache"

subtasks:
  - name: "default"
    task_type: "image_captioning"
    dataset_config: "default"
    metrics: ["bleu", "rougeL"] # Official metrics also include METEOR, CIDEr, SPICE
    input_columns: ["image"]
    label_column: "captions"

evaluation:
  batch_size: 8
  use_cache: true

output:
  format: "json"
  save_raw_predictions: true
  aggregate_subtasks: false
  output_dir: "/root/benchmarks/universal-model-benchmarks/results/raw"
